{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOXm7GrPx3_Z",
        "outputId": "4f275159-6d86-4e2b-b2a0-e9623787eea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fed-conv-social-pooling'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 52 (delta 0), reused 0 (delta 0), pack-reused 49 (from 1)\u001b[K\n",
            "Receiving objects: 100% (52/52), 26.95 KiB | 26.95 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/takakib123/fed-conv-social-pooling.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Datasets/ngsim dataset'"
      ],
      "metadata": {
        "id": "VQQwuZavyBq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/fed-conv-social-pooling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sqlvhb1yOXp",
        "outputId": "bd1d0754-f828-4d0c-b267-b077187c91e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fed-conv-social-pooling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvxGByyDypBQ",
        "outputId": "3e641f36-9701-4e3b-f9d5-1ec8c700c50c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Create a new API key at: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Store your API key securely and do not share it.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste your API key and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makibc123\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import logging\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import wandb\n",
        "\n",
        "# Import necessary modules from the provided file structure\n",
        "from model import highwayNet\n",
        "from utils import ngsimDataset, maskedNLL, maskedMSE, maskedNLLTest\n",
        "\n",
        "# --- Configuration & Logging Setup ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - [Central-System] - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"central_experiment.log\", mode='w'),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "ARGS = {\n",
        "    'use_cuda': True and torch.cuda.is_available(),\n",
        "    'encoder_size': 64,\n",
        "    'decoder_size': 128,\n",
        "    'in_length': 16,\n",
        "    'out_length': 25,\n",
        "    'grid_size': (13, 3),\n",
        "    'soc_conv_depth': 64,\n",
        "    'conv_3x1_depth': 16,\n",
        "    'dyn_embedding_size': 32,\n",
        "    'input_embedding_size': 32,\n",
        "    'num_lat_classes': 3,\n",
        "    'num_lon_classes': 2,\n",
        "    'use_maneuvers': True,\n",
        "    'train_flag': True\n",
        "}\n",
        "\n",
        "\n",
        "TOTAL_EPOCHS = 10\n",
        "PRETRAIN_EPOCHS = 5         # Rounds to use MSE\n",
        "BATCH_SIZE = 2048\n",
        "DEVICE = torch.device(\"cuda\" if ARGS['use_cuda'] else \"cpu\")\n",
        "LOG_INTERVAL = 10           # Log every 10 minibatches\n",
        "VAL_SUBSET_RATIO = 1\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, epoch_num, crossEnt):\n",
        "    \"\"\"\n",
        "    Executes one training epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    model.train_flag = True\n",
        "\n",
        "    epoch_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "\n",
        "    use_mse_loss = epoch_num < PRETRAIN_EPOCHS\n",
        "    loss_mode = \"MSE\" if use_mse_loss else \"NLL\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "        hist, nbrs, mask, lat_enc, lon_enc, fut, op_mask = data\n",
        "\n",
        "        if ARGS['use_cuda']:\n",
        "            hist = hist.to(DEVICE)\n",
        "            nbrs = nbrs.to(DEVICE)\n",
        "            mask = mask.to(DEVICE)\n",
        "            lat_enc = lat_enc.to(DEVICE)\n",
        "            lon_enc = lon_enc.to(DEVICE)\n",
        "            fut = fut.to(DEVICE)\n",
        "            op_mask = op_mask.to(DEVICE)\n",
        "\n",
        "        # Forward pass logic\n",
        "        if ARGS['use_maneuvers']:\n",
        "            fut_pred, lat_pred, lon_pred = model(hist, nbrs, mask, lat_enc, lon_enc)\n",
        "\n",
        "            if use_mse_loss:\n",
        "                l = maskedMSE(fut_pred, fut, op_mask)\n",
        "            else:\n",
        "                l = maskedNLL(fut_pred, fut, op_mask) + \\\n",
        "                    crossEnt(lat_pred, lat_enc) + \\\n",
        "                    crossEnt(lon_pred, lon_enc)\n",
        "        else:\n",
        "            fut_pred = model(hist, nbrs, mask, lat_enc, lon_enc)\n",
        "            if use_mse_loss:\n",
        "                l = maskedMSE(fut_pred, fut, op_mask)\n",
        "            else:\n",
        "                l = maskedNLL(fut_pred, fut, op_mask)\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += l.item()\n",
        "        batch_count += 1\n",
        "\n",
        "        # --- Granular Logging ---\n",
        "        if (i + 1) % LOG_INTERVAL == 0:\n",
        "            log_msg = (f\"Epoch {epoch_num+1} | Batch {i+1} | Loss ({loss_mode}): {l.item():.4f}\")\n",
        "            logger.info(log_msg)\n",
        "\n",
        "            # WandB logging\n",
        "            wandb.log({\n",
        "                \"train_batch_loss\": l.item(),\n",
        "                \"epoch\": epoch_num + 1,\n",
        "                \"batch\": i + 1,\n",
        "                \"training_phase\": loss_mode\n",
        "            })\n",
        "\n",
        "    avg_loss = epoch_loss / batch_count if batch_count > 0 else 0\n",
        "    duration = time.time() - start_time\n",
        "    logger.info(f\"Epoch {epoch_num+1} Complete | Avg Loss: {avg_loss:.4f} | Time: {duration:.2f}s\")\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "def validate(model, val_loader, epoch_num):\n",
        "    \"\"\"\n",
        "    [cite_start]Validation logic mirroring train.py [cite: 85-90].\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.train_flag = False\n",
        "\n",
        "    avg_val_loss = 0\n",
        "    val_batch_count = 0\n",
        "\n",
        "    use_mse_loss = epoch_num < PRETRAIN_EPOCHS\n",
        "    loss_mode = \"MSE\" if use_mse_loss else \"NLL\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            hist, nbrs, mask, lat_enc, lon_enc, fut, op_mask = data\n",
        "\n",
        "            if ARGS['use_cuda']:\n",
        "                hist = hist.to(DEVICE)\n",
        "                nbrs = nbrs.to(DEVICE)\n",
        "                mask = mask.to(DEVICE)\n",
        "                lat_enc = lat_enc.to(DEVICE)\n",
        "                lon_enc = lon_enc.to(DEVICE)\n",
        "                fut = fut.to(DEVICE)\n",
        "                op_mask = op_mask.to(DEVICE)\n",
        "\n",
        "            if ARGS['use_maneuvers']:\n",
        "                if use_mse_loss:\n",
        "                    # Pre-training validation: temporary train_flag=True for MSE output\n",
        "                    model.train_flag = True\n",
        "                    fut_pred, _, _ = model(hist, nbrs, mask, lat_enc, lon_enc)\n",
        "                    l = maskedMSE(fut_pred, fut, op_mask)\n",
        "                    model.train_flag = False\n",
        "                else:\n",
        "                    # NLL Validation\n",
        "                    fut_pred, lat_pred, lon_pred = model(hist, nbrs, mask, lat_enc, lon_enc)\n",
        "                    l = maskedNLLTest(fut_pred, lat_pred, lon_pred, fut, op_mask, avg_along_time=True)\n",
        "            else:\n",
        "                fut_pred = model(hist, nbrs, mask, lat_enc, lon_enc)\n",
        "                if use_mse_loss:\n",
        "                    l = maskedMSE(fut_pred, fut, op_mask)\n",
        "                else:\n",
        "                    l = maskedNLL(fut_pred, fut, op_mask)\n",
        "\n",
        "            avg_val_loss += l.item()\n",
        "            val_batch_count += 1\n",
        "\n",
        "    final_loss = avg_val_loss / val_batch_count if val_batch_count > 0 else 0\n",
        "    return final_loss, loss_mode\n",
        "\n",
        "def main():\n",
        "    # Initialize WandB\n",
        "    wandb.init(\n",
        "        project=\"conv-social-pooling-central\",\n",
        "        reinit=True,\n",
        "        config={\n",
        "            \"total_epochs\": TOTAL_EPOCHS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"pretrain_epochs\": PRETRAIN_EPOCHS,\n",
        "            \"val_subset_ratio\": VAL_SUBSET_RATIO,\n",
        "            **ARGS\n",
        "        }\n",
        "    )\n",
        "\n",
        "    logger.info(\"Initializing Centralized Training Pipeline...\")\n",
        "\n",
        "    # 1. Load Datasets\n",
        "    logger.info(\"Loading Data...\")\n",
        "    try:\n",
        "        train_dataset_ful = ngsimDataset('./data/TrainSet.mat')\n",
        "        val_dataset_full = ngsimDataset('./data/ValSet.mat')\n",
        "    except FileNotFoundError:\n",
        "        logger.error(\"Data files not found. Check 'data/' directory.\")\n",
        "        return\n",
        "    num_train_samples = int(len(train_dataset_ful)/20)\n",
        "    train_dataset = Subset(train_dataset_ful, list(range(num_train_samples)))\n",
        "\n",
        "    # Train Loader\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        collate_fn=train_dataset_ful.collate_fn\n",
        "    )\n",
        "\n",
        "    # Shorten Validation Dataset (10%)\n",
        "    val_len = len(val_dataset_full)\n",
        "    short_val_len = int(val_len * VAL_SUBSET_RATIO)\n",
        "    logger.info(f\"Shortening validation set: {short_val_len} samples (Original: {val_len})\")\n",
        "    val_subset = Subset(val_dataset_full, list(range(short_val_len)))\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        collate_fn=val_dataset_full.collate_fn\n",
        "    )\n",
        "\n",
        "    # 2. Initialize Model & Optimizer\n",
        "    net = highwayNet(ARGS).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    crossEnt = torch.nn.BCELoss()\n",
        "\n",
        "    if not os.path.exists('trained_models'):\n",
        "        os.makedirs('trained_models')\n",
        "\n",
        "    best_val_loss = math.inf\n",
        "\n",
        "    # 3. Training Loop\n",
        "    for epoch in range(TOTAL_EPOCHS):\n",
        "        logger.info(f\"--- Epoch {epoch + 1}/{TOTAL_EPOCHS} ---\")\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_epoch(net, train_loader, optimizer, epoch, crossEnt)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, loss_mode = validate(net, val_loader, epoch)\n",
        "\n",
        "        # Log Summary\n",
        "        logger.info(f\"Epoch {epoch + 1} Summary | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} [{loss_mode}]\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"val_loss\": val_loss,\n",
        "            \"avg_train_loss\": train_loss,\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"training_phase\": loss_mode\n",
        "        })\n",
        "\n",
        "        # Save Best Model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(net.state_dict(), 'trained_models/cslstm_central_best.tar')\n",
        "            logger.info(f\"New best model saved (Loss: {best_val_loss:.4f})\")\n",
        "            wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
        "\n",
        "        # Periodic Checkpoint\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save(net.state_dict(), f'trained_models/central_epoch_{epoch+1}.tar')\n",
        "\n",
        "    logger.info(\"Training Complete.\")\n",
        "    torch.save(net.state_dict(), 'trained_models/cslstm_central_final.tar')\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "9yN8LMqnyPM2",
        "outputId": "6bc235f8-8d59-4055-d946-0cf7146479c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makibc123\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/fed-conv-social-pooling/wandb/run-20260212_174603-e1fuq87t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/akibc123/conv-social-pooling-central/runs/e1fuq87t' target=\"_blank\">logical-water-5</a></strong> to <a href='https://wandb.ai/akibc123/conv-social-pooling-central' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/akibc123/conv-social-pooling-central' target=\"_blank\">https://wandb.ai/akibc123/conv-social-pooling-central</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/akibc123/conv-social-pooling-central/runs/e1fuq87t' target=\"_blank\">https://wandb.ai/akibc123/conv-social-pooling-central/runs/e1fuq87t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▆▅▄▃▁▁▁▁▁</td></tr><tr><td>batch</td><td>▃▄▆█▃▇▁▃▃▄▇▃▆▆▃▅▆▇█▂▄▆▇▁▃▅▆▇█▂▆▇█▃▄▇▇▂▆█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_batch_loss</td><td>██▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▃▃▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>5.69558</td></tr><tr><td>batch</td><td>140</td></tr><tr><td>best_val_loss</td><td>5.23793</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>train_batch_loss</td><td>5.6797</td></tr><tr><td>training_phase</td><td>NLL</td></tr><tr><td>val_loss</td><td>5.23793</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">logical-water-5</strong> at: <a href='https://wandb.ai/akibc123/conv-social-pooling-central/runs/e1fuq87t' target=\"_blank\">https://wandb.ai/akibc123/conv-social-pooling-central/runs/e1fuq87t</a><br> View project at: <a href='https://wandb.ai/akibc123/conv-social-pooling-central' target=\"_blank\">https://wandb.ai/akibc123/conv-social-pooling-central</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260212_174603-e1fuq87t/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}